{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")\n",
    "train_X = torch.tensor(mnist.data[:60000], dtype=torch.float) / 255\n",
    "train_Y = torch.tensor([int(x) for x in mnist.target[:60000]])\n",
    "val_X = torch.tensor(mnist.data[60000:], dtype=torch.float) / 255\n",
    "val_Y = torch.tensor([int(x) for x in mnist.target[60000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "tensor(2.0786, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  2\n",
      "tensor(1.7401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  3\n",
      "tensor(1.3541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  4\n",
      "tensor(0.8524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  5\n",
      "tensor(0.5628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  6\n",
      "tensor(0.4400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  7\n",
      "tensor(0.3769, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  8\n",
      "tensor(0.3325, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  9\n",
      "tensor(0.2994, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  10\n",
      "tensor(0.2732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  11\n",
      "tensor(0.2513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  12\n",
      "tensor(0.2329, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  13\n",
      "tensor(0.2171, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  14\n",
      "tensor(0.2033, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  15\n",
      "tensor(0.1910, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  16\n",
      "tensor(0.1803, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  17\n",
      "tensor(0.1708, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  18\n",
      "tensor(0.1625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  19\n",
      "tensor(0.1553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  20\n",
      "tensor(0.1491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  21\n",
      "tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  22\n",
      "tensor(0.1385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  23\n",
      "tensor(0.1339, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  24\n",
      "tensor(0.1296, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  25\n",
      "tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  26\n",
      "tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  27\n",
      "tensor(0.1180, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  28\n",
      "tensor(0.1141, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  29\n",
      "tensor(0.1102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  30\n",
      "tensor(0.1067, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  31\n",
      "tensor(0.1033, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  32\n",
      "tensor(0.1003, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  33\n",
      "tensor(0.0974, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  34\n",
      "tensor(0.0946, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  35\n",
      "tensor(0.0920, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  36\n",
      "tensor(0.0896, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  37\n",
      "tensor(0.0872, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  38\n",
      "tensor(0.0851, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  39\n",
      "tensor(0.0830, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  40\n",
      "tensor(0.0810, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  41\n",
      "tensor(0.0791, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  42\n",
      "tensor(0.0772, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  43\n",
      "tensor(0.0753, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  44\n",
      "tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  45\n",
      "tensor(0.0718, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  46\n",
      "tensor(0.0701, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  47\n",
      "tensor(0.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  48\n",
      "tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  49\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  50\n",
      "tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  51\n",
      "tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  52\n",
      "tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  53\n",
      "tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  54\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  55\n",
      "tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  56\n",
      "tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  57\n",
      "tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  58\n",
      "tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  59\n",
      "tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  60\n",
      "tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  61\n",
      "tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  62\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  63\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  64\n",
      "tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  65\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  66\n",
      "tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  67\n",
      "tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  68\n",
      "tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  69\n",
      "tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  70\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  71\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  72\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  73\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  74\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  75\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  76\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  77\n",
      "tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  78\n",
      "tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  79\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  80\n",
      "tensor(0.0371, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  81\n",
      "tensor(0.0359, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  82\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  83\n",
      "tensor(0.0334, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  84\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  85\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  86\n",
      "tensor(0.0306, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  87\n",
      "tensor(0.0299, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  88\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  89\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  90\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  91\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  92\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  93\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  94\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  95\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  96\n",
      "tensor(0.0271, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  97\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  98\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  99\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "epoch :  100\n",
      "tensor(0.0293, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "train_X = train_X.view(60000, 1, 28, 28)\n",
    "val_X = val_X.view(10000, 1, 28, 28)\n",
    "\n",
    "batch_size = 100\n",
    "lr = 0.0001\n",
    "epoch = 100\n",
    "train_X = train_X.view(batch_size, -1, 28, 28)\n",
    "train_Y = train_Y.view(batch_size, -1)\n",
    "\n",
    "LayerBlock = namedtuple('LayerBlock', ['num_repeats', 'num_filters', 'bottleneck_size'])\n",
    "blocks = [LayerBlock(3, 128, 32), LayerBlock(3, 256, 64), LayerBlock(3, 512, 128), LayerBlock(3, 1024, 256)]\n",
    "\n",
    "class Layers(nn.Module):\n",
    "    def __init__(self, filter, bottleneck):\n",
    "        super(Layers, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(filter, bottleneck, kernel_size=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(bottleneck, bottleneck, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(bottleneck, filter, kernel_size=(1, 1)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.upscale = nn.Conv2d(filter, filter*2, kernel_size=(1, 1), bias=False)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.ready = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(8, 8), stride=2, padding=2),#14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=1),  #7, 7\n",
    "            nn.Conv2d(64, block[0].num_filters, kernel_size=(1, 1)) #7, 7\n",
    "        )\n",
    "        self.layer1 = Layers(block[0].num_filters, block[0].bottleneck_size)\n",
    "        self.layer2 = Layers(block[1].num_filters, block[1].bottleneck_size)\n",
    "        self.layer3 = Layers(block[2].num_filters, block[2].bottleneck_size)\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        out = self.ready(x)\n",
    "        out1 = self.layer1.layers(out)\n",
    "        output = out + out1\n",
    "        out = self.layer1.upscale(output)\n",
    "        out2 = self.layer2.layers(out)\n",
    "        output = out + out2\n",
    "        out = self.layer2.upscale(output)\n",
    "        out3 = self.layer3.layers(out)\n",
    "        output = out + out3\n",
    "        out = self.layer3.upscale(output)\n",
    "        out = nn.functional.avg_pool2d(out, kernel_size=(out.size()[2], out.size()[3]))\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(blocks)\n",
    "model = model.cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "for ep in range(epoch):\n",
    "    print('epoch : ', ep+1)\n",
    "    avg_cost = 0\n",
    "    for batch in range(batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        X = train_X[batch]\n",
    "        X = X.unsqueeze(1).cuda()\n",
    "        y_pred = model.forward(X).cuda()\n",
    "        cost = loss(y_pred, train_Y[batch].cuda())\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost\n",
    "        torch.cuda.empty_cache()\n",
    "    print(avg_cost / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test정확도 :  98.26333333333334\n"
     ]
    }
   ],
   "source": [
    "T, F = 0, 0\n",
    "for i in range(batch_size):\n",
    "    X = train_X[i].unsqueeze(1).cuda()\n",
    "    y_pred = model.forward(X).argmax(axis=1)\n",
    "    Y = train_Y[i].cuda()\n",
    "    for j in range(len(y_pred)):\n",
    "        if y_pred[j] == Y[j]:\n",
    "            T += 1\n",
    "        else:\n",
    "            F += 1\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Test정확도 : \",T / (T+F) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation정확도 :  98.00999999999999\n"
     ]
    }
   ],
   "source": [
    "val_X = val_X.view(batch_size, -1, 28, 28)\n",
    "val_Y = val_Y.view(batch_size, -1)\n",
    "T, F = 0, 0\n",
    "for i in range(batch_size):\n",
    "    test_X = val_X[i].unsqueeze(1).cuda()\n",
    "    test_pred = model.forward(test_X).argmax(axis=1)\n",
    "    test_Y = val_Y[i].cuda()\n",
    "    for j in range(len(test_pred)):\n",
    "        if test_pred[j] == test_Y[j]:\n",
    "            T += 1\n",
    "        else:\n",
    "            F += 1\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Validation정확도 : \", T / (T+F) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
